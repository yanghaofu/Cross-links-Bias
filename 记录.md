## 结构
1. augmentation.py：增强算法，true_aug可以选0，即随机选边，不一定是社区间的。

## 细节
1. data['train'][0]为社区内的边


## 消融实验
所有的结果都是基于基于jaccard的增强。都是基于LightGCN。
![alt text](img/image.png)
1. 监督增强对我们的框架的影响；
2. 嵌入融合模块对我们的框架的影响；
3. 等式中两个辅助损失函数的影响；
4. 训练嵌入融合模块过程中动态训练策略的影响。

### -Augment
将通过Jaccard系数选择的增强的交联信号替换为跨不同社区的随机节点对。
true_aug=0，进行虚假增强，随机选择前augment_size条增强边

epinions: Group 0 hit@50: 0.5225, Group 1 hit@50: 0.4599, Overall hit@50: 0.5002

### -Fusion
model_cal=AverageMerger()
以一种简单的方式将ZA融合到ZO中，简单的平均。
epinions: Group 0 hit@50: 0.4734, Group 1 hit@50: 0.3773, Overall hit@50: 0.4392, 

### -Auxiliary Loss
注释掉e2e_train的前半部分对双GNN的训练过程

### -Dynamic
设置一个固定的学习速率γ和训练步骤S
~~~python
LR = args.alpha * 1 / (1 + math.exp(-EPOCH + args.threshold))
STEP = int(args.beta * 1 / (1 + math.exp(-EPOCH + args.threshold))) + 1
~~~

## 对比
我将使用LightGCN与论文方法进行比较。

### LightGCN简述
在LightGCN中采用了简单的加权和聚集器，放弃了特征变换和非线性激活。LightGCN中图卷积运算为：
$$
\mathbf{e}_\mathrm{u}^{(\mathrm{k}+1)}\:=\:\sum_{\mathrm{i}\in\mathcal{N}_\mathrm{u}}\:\frac{1}{\sqrt{\mid\mathcal{N}_\mathrm{u}\mid\mid\mathcal{N}\mathrm{i}\mid}}\:\mathbf{e}_\mathrm{i}^{(\mathrm{k})}
$$

$$\mathbf{e}_\mathrm{i}^{(\mathrm{k}+1)}\:=\:\sum_{\mathrm{u}\in\mathcal{N}_\mathrm{i}}\:\frac{1}{\sqrt{\mid\mathcal{N}_\mathrm{u}\mid\mid\mathcal{N}\mathrm{i}\mid}}\:\mathbf{e}_\mathrm{u}^{(\mathrm{k})}$$

在LightGCN中，**唯一可训练的模型参数是第0层的嵌入**，即$\mathrm{e}_\mathrm{u}^{0}$表示所有用户，$\mathbf{e}_\mathrm{v}^{0}$表示所有项目。当给定它们时，就可以通过上述公式中定义的LGC计算更高层的嵌入。经过K层LGC之后，进一步结合在每一层得到的嵌入，形成一个用户(一个项目)的最终表示
$$\mathrm{e_u~=\sum_{k=0}^K\alpha_k~e_u^{(k)};  e_i~=\sum_{k=1}^K\alpha_k~e_i^{(k)}}$$

其中$\alpha_\mathrm{k}\geq0$表示第k层嵌入的权重。作者在实验中发现设置$\mathfrak{\alpha}_\mathrm{k}$统一为$1/(\mathbb{K}+1)$的性能较好。


最后，模型预测定义为用户和项目最终表示的内积：
$$\hat{\mathrm{y}}_\mathrm{ui}=\mathrm{e}_\mathrm{u}^\mathrm{T}\mathrm{e}_\mathrm{i}$$

